{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image, ImageEnhance, ImageOps\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras._tf_keras.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += residual\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),  # Changed back to 64 channels\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 128, 3, padding=1),  # Changed back to 128 channels\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        \n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            ResidualBlock(128),  # Changed back to 128 channels\n",
    "            ResidualBlock(128),\n",
    "            ResidualBlock(128)   # Added back the third residual block\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True),\n",
    "            nn.Conv2d(128, 64, 3, padding=1),  # Changed to match checkpoint\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),   # Changed to match checkpoint\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 3, 3, padding=1),    # Changed to match checkpoint\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.residual_blocks(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedDRDataset(Dataset):\n",
    "    def __init__(self, df, base_image_dir, label_mapping, generator_model, device, img_size=(224, 224)):\n",
    "        self.df = df\n",
    "        self.base_image_dir = base_image_dir\n",
    "        self.label_mapping = label_mapping\n",
    "        self.img_size = img_size\n",
    "        self.generator = generator_model\n",
    "        self.device = device\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(256),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                               std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "    def enhance_image(self, image):\n",
    "        # Convert to PIL Image if it's not already\n",
    "        if not isinstance(image, Image.Image):\n",
    "            image = Image.fromarray(image)\n",
    "            \n",
    "        # Apply GAN enhancement\n",
    "        img_tensor = self.transform(image).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            enhanced = self.generator(img_tensor)\n",
    "        \n",
    "        # Convert back to image\n",
    "        enhanced = enhanced.squeeze(0).cpu()\n",
    "        enhanced = enhanced * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + \\\n",
    "                  torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "        enhanced = enhanced.clamp(0, 1)\n",
    "        enhanced = transforms.ToPILImage()(enhanced)\n",
    "        \n",
    "        # Adjust enhancement parameters\n",
    "        enhanced = ImageEnhance.Brightness(enhanced).enhance(1.1)  # Reduced enhancement\n",
    "        enhanced = ImageEnhance.Contrast(enhanced).enhance(1.6)     # Reduced enhancement\n",
    "        enhanced = ImageEnhance.Sharpness(enhanced).enhance(1.7)    # Reduced enhancement\n",
    "\n",
    "        return enhanced\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        label = row['diagnosis']\n",
    "        \n",
    "        # Find the image in the appropriate directory\n",
    "        for label_name, label_num in self.label_mapping.items():\n",
    "            img_dir = os.path.join(self.base_image_dir, label_name)\n",
    "            img_path = os.path.join(img_dir, row['id_code'] + '.png')\n",
    "            if os.path.exists(img_path):\n",
    "                # Load and enhance the image\n",
    "                image = Image.open(img_path).convert('RGB')\n",
    "                enhanced_image = self.enhance_image(image)\n",
    "                \n",
    "                # Convert to numpy array and normalize\n",
    "                enhanced_array = np.array(enhanced_image)\n",
    "                enhanced_array = enhanced_array / 255.0\n",
    "                \n",
    "                return enhanced_array, label\n",
    "                \n",
    "        raise FileNotFoundError(f\"Image not found for id_code: {row['id_code']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data_with_gan(df, base_image_dir, label_mapping, generator_model, device, img_size=(224, 224)):\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Create an instance of EnhancedDRDataset for image enhancement\n",
    "    dataset = EnhancedDRDataset(df, base_image_dir, label_mapping, generator_model, device)\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        for label_name, label_num in label_mapping.items():\n",
    "            img_dir = os.path.join(base_image_dir, label_name)\n",
    "            img_path = os.path.join(img_dir, row['id_code'] + '.png')\n",
    "            if os.path.exists(img_path):\n",
    "                # Get enhanced image using GAN\n",
    "                image, _ = dataset[index]\n",
    "                \n",
    "                # Convert to cv2 format and resize\n",
    "                image = (image * 255).astype(np.uint8)\n",
    "                image = cv2.resize(image, img_size)\n",
    "                \n",
    "                images.append(image)\n",
    "                labels.append(label_num)\n",
    "\n",
    "    images = np.array(images)\n",
    "    labels = np.array(labels)\n",
    "    labels = to_categorical(labels, num_classes=5)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras._tf_keras.keras.models import Sequential, Model\n",
    "from keras._tf_keras.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from keras._tf_keras.keras.layers import (\n",
    "    Conv2D, \n",
    "    MaxPooling2D, \n",
    "    Dense, \n",
    "    Flatten, \n",
    "    Dropout,\n",
    "    BatchNormalization\n",
    ")\n",
    "from keras._tf_keras.keras.models import Sequential\n",
    "\n",
    "def create_cnn_model(input_shape=(224, 224, 3)):\n",
    "    model = Sequential([\n",
    "        # First Convolutional Block\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Second Convolutional Block\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Third Convolutional Block\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Dense layers\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(128, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Dense(5, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def train_combined_models(df, base_image_dir, label_mapping, gan_model_path='enhanced_gan_models.pth'):\n",
    "    # Set up device for GAN\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load and initialize the GAN generator\n",
    "    generator = Generator().to(device)\n",
    "    checkpoint = torch.load(gan_model_path, map_location=device)\n",
    "    generator.load_state_dict(checkpoint['model_state_dict'])\n",
    "    generator.eval()\n",
    "    \n",
    "    # Preprocess data with GAN enhancement\n",
    "    print(\"Preprocessing data with GAN enhancement...\")\n",
    "    images, labels = preprocess_data_with_gan(df, base_image_dir, label_mapping, generator, device)\n",
    "    \n",
    "    # Split the data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Create and compile CNN model\n",
    "    cnn_model = create_cnn_model()\n",
    "    cnn_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),  # Lower learning rate\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    # Add data augmentation\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        tf.keras.layers.RandomRotation(0.1),\n",
    "        tf.keras.layers.RandomZoom(0.1),\n",
    "        tf.keras.layers.RandomFlip(\"horizontal\")\n",
    "    ])\n",
    "    \n",
    "    # Train with more epochs and adjusted batch size\n",
    "    history = cnn_model.fit(\n",
    "        data_augmentation(X_train), y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=10,  # Increased epochs\n",
    "        # batch_size=32,  # Adjusted batch size\n",
    "    )\n",
    "    \n",
    "    return cnn_model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your data\n",
    "data_path = 'FGADR/train.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "base_image_dir = 'FGADR'\n",
    "\n",
    "# Define label mapping\n",
    "label_mapping = {\n",
    "    '0': 0,\n",
    "    '1': 1,\n",
    "    '2': 2,\n",
    "    '3': 3,\n",
    "    '4': 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data with GAN enhancement...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ompat\\Downloads\\DRD_GAN\\venv\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 2s/step - accuracy: 0.3536 - loss: 1.9339 - val_accuracy: 0.6385 - val_loss: 1.4845\n",
      "Epoch 2/10\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 2s/step - accuracy: 0.5362 - loss: 1.3354 - val_accuracy: 0.6739 - val_loss: 1.3858\n",
      "Epoch 3/10\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m171s\u001b[0m 2s/step - accuracy: 0.5573 - loss: 1.2449 - val_accuracy: 0.5375 - val_loss: 1.6564\n",
      "Epoch 4/10\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m167s\u001b[0m 2s/step - accuracy: 0.5976 - loss: 1.1237 - val_accuracy: 0.6971 - val_loss: 0.9224\n",
      "Epoch 5/10\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m168s\u001b[0m 2s/step - accuracy: 0.6602 - loss: 0.9905 - val_accuracy: 0.4379 - val_loss: 1.5341\n",
      "Epoch 6/10\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 2s/step - accuracy: 0.6728 - loss: 0.9235 - val_accuracy: 0.6398 - val_loss: 0.9291\n",
      "Epoch 7/10\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m163s\u001b[0m 2s/step - accuracy: 0.6949 - loss: 0.8783 - val_accuracy: 0.5812 - val_loss: 1.2774\n",
      "Epoch 8/10\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m159s\u001b[0m 2s/step - accuracy: 0.6685 - loss: 0.9136 - val_accuracy: 0.7231 - val_loss: 0.9519\n",
      "Epoch 9/10\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 2s/step - accuracy: 0.6992 - loss: 0.8541 - val_accuracy: 0.5580 - val_loss: 1.0891\n",
      "Epoch 10/10\n",
      "\u001b[1m92/92\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m157s\u001b[0m 2s/step - accuracy: 0.7029 - loss: 0.8021 - val_accuracy: 0.6180 - val_loss: 1.3013\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the models\n",
    "cnn_model, history = train_combined_models(\n",
    "    df=df,\n",
    "    base_image_dir=base_image_dir,\n",
    "    label_mapping=label_mapping,\n",
    "    gan_model_path='enhanced_gan_models.pth'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
